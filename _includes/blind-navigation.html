<h2>Blind Navigation for Visual Impairement People</h2>
    <ul class="project-list">
         <li class="project-item" data-project-id="ai-forestry">
            <div class="project-content">
                <img src="assets/images/ego-motion/display.png" alt="Tracking">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Wearable ego-motion tracking for blind navigation in indoor environments 
                        </h3>
                    </div>
                    <p>This paper proposes a method to measure the motion of a moving rigid body using a hybrid visual–inertial sensor. The rotational velocity of the moving object is computed from visual optical flow by solving a depthindependent bilinear constraint, and the translational velocity of the moving object is estimated by solving a dynamics constraint that reveals the relation between scene depth and translational motion. By fusing an inertial sensor, the scale of translational velocities can be estimated, which is otherwise unrecoverable from monocular visual optical flow. An iterative refinement scheme is introduced to deal with observation noise and outliers, and the extended Kalman filter is applied for motion tracking. The performance of the proposed method is evaluated by simulation studies and practical experiments, and the results show the effectiveness of the proposed method in terms of accuracy and robustness.</p>
                    <div class="tech-list">
                        <span class="tech-item">Computer Vision</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Ego motion tracking</span>
                        <span class="tech-item">Feature scarcity scene tracking</span>
                    </div>
                </div>
            </div>
        </li>
        <!-- Project 2 -->
        <li class="project-item" data-project-id="ai-forestry">
            <div class="project-content">
                <img src="assets/images/dense-scene-depth/display.png" alt="Scene-depth">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Real-scale dense scene depth estimation based on visual-inertial measurements 
                        </h3>
                    </div>
                    <p>This paper proposes a method to measure the motion of a moving rigid body using a hybrid visual–inertial sensor. The rotational velocity of the moving object is computed from visual optical flow by solving a depthindependent bilinear constraint, and the translational velocity of the moving object is estimated by solving a dynamics constraint that reveals the relation between scene depth and translational motion. By fusing an inertial sensor, the scale of translational velocities can be estimated, which is otherwise unrecoverable from monocular visual optical flow. An iterative refinement scheme is introduced to deal with observation noise and outliers, and the extended Kalman filter is applied for motion tracking. The performance of the proposed method is evaluated by simulation studies and practical experiments, and the results show the effectiveness of the proposed method in terms of accuracy and robustness. </p>
                    <div class="tech-list">
                        <span class="tech-item">Real-scale depth estimation</span>
                        <span class="tech-item">Dense visual scene depth</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Dense Optical flow</span>
                    </div>
                </div>
            </div>
        </li>
        <!-- Project 3 -->
         <li class="project-item" data-project-id="ai-forestry">
            <div class="project-content">
                <img src="assets/images/relative-motion/display.png" alt="relative-motion">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Relative motion estimation using visual-inertial optical flow 
                        </h3>
                        <p>This paper proposes a method to measure the motion of a moving rigid body using a hybrid visual-inertial sensor. The rotational velocity of the moving object is computed from visual optical flow by solving a depthindependent bilinear constraint, and the translational velocity of the moving object is estimated by solving a dynamics constraint that reveals the relation between scene depth and translational motion. By fusing an inertial sensor, the scale of translational velocities can be estimated, which is otherwise unrecoverable from monocular visual optical flow. An iterative refinement scheme is introduced to deal with observation noise and outliers, and the extended Kalman filter is applied for motion tracking. The performance of the proposed method is evaluated by simulation studies and practical experiments, and the results show the effectiveness of the proposed method in terms of accuracy and robustness.</p>
                    </div><div class="tech-list">
                        <span class="tech-item">Real-scale movement estimation</span>
                        <span class="tech-item">Visual scene depth</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Optical flow</span>
                    </div>
                </div>
            </div>
        </li>
        <!-- Project 4 -->
         <li class="project-item" data-project-id="ai-forestry">
            <div class="project-content">
                <img src="assets/images/rotation-calibration/display.png" alt="calibration">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Rotational coordinate transformation for visual-inertial sensor fusion 
                        </h3>
                        <p>Visual and inertial sensors are used collaboratively in many applications because of their complementary properties. The problem associated with sensor fusion is relative coordinate transformations. This paper presents a quaternion-based method to estimate the relative rotation between visual and inertial sensors. Rotation between a camera and an inertial measurement unit (IMU) is represented by quaternions, which are separately measured to allow the sensor to be optimized individually. Relative quaternions are used so that the global reference is not required to be known. The accuracy of the coordinate transformation was evaluated by comparing with a ground-truth tracking system. The experiment analysis proves the effectiveness of the proposed method in terms of accuracy and robustness.</p>
                    </div><div class="tech-list">
                        <span class="tech-item">Real-scale depth estimation</span>
                        <span class="tech-item">Visual scene depth</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Optical flow</span>
                    </div>
                </div>
            </div>
        </li>
    </ul>
