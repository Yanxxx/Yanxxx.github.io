<h2>Blind Navigation for Visual Impairement People</h2>
    <ul class="project-list">
         <li class="project-item" data-project-id="ego-motion">
            <div class="project-content">
                <img src="assets/images/ego-motion/main.png" alt="Tracking">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Wearable ego-motion tracking for blind navigation in indoor environments 
                        </h3>
                    </div>
                    <p>
                        Proposed a Robust Visual-Inertial Odometry Framework with an Inertial-Based 
                        Sanity Check for Graceful Degradation in Feature-Scarce Environments.                    
                    </p>
                    <div class="tech-list">
                        <span class="tech-item">Computer Vision</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Ego motion tracking</span>
                        <span class="tech-item">Feature scarcity scene tracking</span>
                    </div>
                </div>
            </div>
        </li>
        <!-- Project 2 -->
        <li class="project-item" data-project-id="depth">
            <div class="project-content">
                <img src="assets/images/dense-scene-depth/display.png" alt="Scene-depth">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Real-scale dense scene depth estimation based on visual-inertial measurements 
                        </h3>
                    </div>
                    <p>
                        Formulated a method to resolve the inherent scale ambiguity of visual estimations by integrating them with inertial data.
                    </p>
                    <div class="tech-list">
                        <span class="tech-item">Real-scale depth estimation</span>
                        <span class="tech-item">Dense visual scene depth</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Dense Optical flow</span>
                    </div>
                </div>
            </div>
        </li>
        <!-- Project 3 -->
         <li class="project-item" data-project-id="relative-motion">
            <div class="project-content">
                <img src="assets/images/relative-motion/display.png" alt="relative-motion">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Relative motion estimation using visual-inertial optical flow 
                        </h3>
                        <p>
                            Proposed a Tightly-Coupled, Dense Optical Flow and Inertial Fusion Framework for Robust Relative Motion Estimation
                        </p>
                    </div><div class="tech-list">
                        <span class="tech-item">Real-scale movement estimation</span>
                        <span class="tech-item">Visual scene depth</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Optical flow</span>
                    </div>
                </div>
            </div>
        </li>
        <!-- Project 4 -->
         <li class="project-item" data-project-id="rotation-calibration1">
            <div class="project-content">
                <img src="assets/images/rotation-calibration/display.png" alt="calibration">
                <div class="project-details">
                    <div class="project-title">
                        <h3>
                            Rotational coordinate transformation for visual-inertial sensor fusion 
                        </h3>
                        <p>Visual and inertial sensors are used collaboratively in many applications because of their complementary properties. The problem associated with sensor fusion is relative coordinate transformations. This paper presents a quaternion-based method to estimate the relative rotation between visual and inertial sensors. Rotation between a camera and an inertial measurement unit (IMU) is represented by quaternions, which are separately measured to allow the sensor to be optimized individually. Relative quaternions are used so that the global reference is not required to be known. The accuracy of the coordinate transformation was evaluated by comparing with a ground-truth tracking system. The experiment analysis proves the effectiveness of the proposed method in terms of accuracy and robustness.</p>
                    </div><div class="tech-list">
                        <span class="tech-item">Rotational Coordinate Calibration</span>
                        <span class="tech-item">Fast Calibration</span>
                        <span class="tech-item">IMU</span>
                        <span class="tech-item">Rotation Matrix</span>
                    </div>
                </div>
            </div>
        </li>
    </ul>
